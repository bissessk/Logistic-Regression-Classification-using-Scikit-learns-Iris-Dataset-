{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model\n",
    "# Kieran Bissessar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import matplotlib.pyplot as plt;\n",
    "from sklearn.linear_model import LogisticRegression;\n",
    "from sklearn import datasets;\n",
    "import numpy as np;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris();\n",
    "X = iris.data;\n",
    "y = iris.target;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Six Cases (2 features at a Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ab = iris.data[:, [0, 1]]; # sepal length vs sepal width\n",
    "X_bc = iris.data[:, [1, 2]]; # sepal width  vs petal length\n",
    "X_cd = iris.data[:, [2, 3]]; # petal length vs petal width \n",
    "X_ac = iris.data[:, [0, 2]]; # sepal length vs petal length\n",
    "X_ad = iris.data[:, [0, 3]]; # sepal length vs petal width\n",
    "X_bd = iris.data[:, [1, 3]]; # sepal width  vs petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "Xs_ab = scale(X_ab);\n",
    "Xs_bc = scale(X_bc);\n",
    "Xs_cd = scale(X_cd);\n",
    "Xs_ac = scale(X_ac);\n",
    "Xs_ad = scale(X_ad);\n",
    "Xs_bd = scale(X_bd);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ab = LogisticRegression(solver = \"liblinear\", C=1.0);\n",
    "lr_ab.fit(Xs_ab, y);\n",
    "\n",
    "lr_bc = LogisticRegression(solver = \"liblinear\", C=1.0);\n",
    "lr_bc.fit(Xs_bc, y);\n",
    "\n",
    "lr_cd = LogisticRegression(solver = \"liblinear\", C=1.0);\n",
    "lr_cd.fit(Xs_cd, y);\n",
    "\n",
    "lr_ac = LogisticRegression(solver = \"liblinear\", C=1.0);\n",
    "lr_ac.fit(Xs_ac, y);\n",
    "\n",
    "lr_ad = LogisticRegression(solver = \"liblinear\", C=1.0);\n",
    "lr_ad.fit(Xs_ad, y);\n",
    "\n",
    "lr_bd = LogisticRegression(solver = \"liblinear\", C=1.0);\n",
    "lr_bd.fit(Xs_bd, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuaracy ab :  0.7933333333333333\n",
      "accuaracy bc :  0.86\n",
      "accuaracy cd :  0.94\n",
      "accuaracy ac :  0.8733333333333333\n",
      "accuaracy ad :  0.8866666666666667\n",
      "accuaracy bd :  0.8933333333333333\n"
     ]
    }
   ],
   "source": [
    "y_ab_hat = lr_ab.predict(Xs_ab);\n",
    "accuracy_ab = np.mean(y_ab_hat == y);\n",
    "print(\"accuaracy ab : \", accuracy_ab);\n",
    "\n",
    "y_bc_hat = lr_bc.predict(Xs_bc);\n",
    "accuracy_bc = np.mean(y_bc_hat == y);\n",
    "print(\"accuaracy bc : \", accuracy_bc);\n",
    "\n",
    "y_cd_hat = lr_cd.predict(Xs_cd);\n",
    "accuracy_cd = np.mean(y_cd_hat == y);\n",
    "print(\"accuaracy cd : \", accuracy_cd);\n",
    "\n",
    "y_ac_hat = lr_ac.predict(Xs_ac);\n",
    "accuracy_ac = np.mean(y_ac_hat == y);\n",
    "print(\"accuaracy ac : \", accuracy_ac);\n",
    "\n",
    "y_ad_hat = lr_ad.predict(Xs_ad);\n",
    "accuracy_ad = np.mean(y_ad_hat == y);\n",
    "print(\"accuaracy ad : \", accuracy_ad);\n",
    "\n",
    "y_bd_hat = lr_bd.predict(Xs_bd);\n",
    "accuracy_bd = np.mean(y_bd_hat == y);\n",
    "print(\"accuaracy bd : \", accuracy_bd);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Cases (3 features at a Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_abc = iris.data[:, [0, 1, 2]];\n",
    "X_acd = iris.data[:, [0, 2, 3]];\n",
    "X_abd = iris.data[:, [0, 1, 3]];\n",
    "X_bcd = iris.data[:, [1, 2, 3]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_abc = scale(X_abc);\n",
    "Xs_acd = scale(X_acd);\n",
    "Xs_abd = scale(X_abd);\n",
    "Xs_bcd = scale(X_bcd);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_abc = LogisticRegression(solver = \"liblinear\",C=1.0);\n",
    "lr_abc.fit(Xs_abc, y);\n",
    "\n",
    "lr_acd = LogisticRegression(solver = \"liblinear\",C=1.0);\n",
    "lr_acd.fit(Xs_acd, y);\n",
    "\n",
    "lr_abd = LogisticRegression(solver = \"liblinear\",C=1.0);\n",
    "lr_abd.fit(Xs_abd, y);\n",
    "\n",
    "lr_bcd = LogisticRegression(solver = \"liblinear\",C=1.0);\n",
    "lr_bcd.fit(Xs_bcd, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuaracy abc :  0.8666666666666667\n",
      "accuaracy acd :  0.9133333333333333\n",
      "accuaracy abd :  0.9066666666666666\n",
      "accuaracy bcd :  0.92\n"
     ]
    }
   ],
   "source": [
    "y_abc_hat = lr_abc.predict(Xs_abc);\n",
    "accuracy_abc = np.mean(y_abc_hat == y);\n",
    "print(\"accuaracy abc : \", accuracy_abc);\n",
    "\n",
    "y_acd_hat = lr_acd.predict(Xs_acd);\n",
    "accuracy_acd = np.mean(y_acd_hat == y);\n",
    "print(\"accuaracy acd : \", accuracy_acd);\n",
    "\n",
    "y_abd_hat = lr_abd.predict(Xs_abd);\n",
    "accuracy_abd = np.mean(y_abd_hat == y);\n",
    "print(\"accuaracy abd : \", accuracy_abd);\n",
    "\n",
    "y_bcd_hat = lr_bcd.predict(Xs_bcd);\n",
    "accuracy_bcd = np.mean(y_bcd_hat == y);\n",
    "print(\"accuaracy bcd : \", accuracy_bcd);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Case (4 features at a Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_abcd = iris.data[:, [0, 1, 2, 3]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_abcd = scale(X_abcd);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_abcd = LogisticRegression(solver = \"liblinear\", C=1.0);\n",
    "lr_abcd.fit(Xs_abcd, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuaracy abcd :  0.9266666666666666\n"
     ]
    }
   ],
   "source": [
    "y_abcd_hat = lr_abcd.predict(Xs_abcd);\n",
    "accuracy_abcd = np.mean(y_abcd_hat == y);\n",
    "print(\"accuaracy abcd : \", accuracy_abcd);\n",
    "# print(lr_abcd.score(Xs_abcd,y)) # could also use this to determine accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUMMARY :** The case with all 4 features used had the highest accuracy. The cases using three features were more accurate than the cases using 2 features. Also it seems that teh number of iterations are the same no matter the number of features used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies : \n",
      "accuaracy ab :  0.7933333333333333\n",
      "accuaracy bc :  0.86\n",
      "accuaracy cd :  0.94\n",
      "accuaracy ac :  0.8733333333333333\n",
      "accuaracy ad :  0.8866666666666667\n",
      "accuaracy bd :  0.8933333333333333\n",
      "\n",
      "accuaracy abc :  0.8666666666666667\n",
      "accuaracy acd :  0.9133333333333333\n",
      "accuaracy abd :  0.9066666666666666\n",
      "accuaracy bcd :  0.92\n",
      "\n",
      "accuaracy abcd :  0.9266666666666666\n"
     ]
    }
   ],
   "source": [
    "# Accuracies\n",
    "print(\"Accuracies : \")\n",
    "print(\"accuaracy ab : \", accuracy_ab);\n",
    "print(\"accuaracy bc : \", accuracy_bc);\n",
    "print(\"accuaracy cd : \", accuracy_cd);\n",
    "print(\"accuaracy ac : \", accuracy_ac);\n",
    "print(\"accuaracy ad : \", accuracy_ad);\n",
    "print(\"accuaracy bd : \", accuracy_bd);\n",
    "print();\n",
    "print(\"accuaracy abc : \", accuracy_abc);\n",
    "print(\"accuaracy acd : \", accuracy_acd);\n",
    "print(\"accuaracy abd : \", accuracy_abd);\n",
    "print(\"accuaracy bcd : \", accuracy_bcd);\n",
    "print();\n",
    "print(\"accuaracy abcd : \", accuracy_abcd);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Iterations : \n",
      "ab : [6]\n",
      "bc : [6]\n",
      "cd : [6]\n",
      "ac : [6]\n",
      "ad : [6]\n",
      "bd : [6]\n",
      "\n",
      "abc : [6]\n",
      "acd : [7]\n",
      "abd : [6]\n",
      "bcd : [6]\n",
      "\n",
      "abcd : [6]\n"
     ]
    }
   ],
   "source": [
    "# Number of Iterations\n",
    "\n",
    "print(\"Number of Iterations : \")\n",
    "print(\"ab :\", lr_ab.n_iter_);\n",
    "print(\"bc :\", lr_bc.n_iter_);\n",
    "print(\"cd :\", lr_cd.n_iter_);\n",
    "print(\"ac :\", lr_ac.n_iter_);\n",
    "print(\"ad :\", lr_ad.n_iter_);\n",
    "print(\"bd :\", lr_bd.n_iter_);\n",
    "print();\n",
    "print(\"abc :\", lr_abc.n_iter_);\n",
    "print(\"acd :\", lr_acd.n_iter_);\n",
    "print(\"abd :\", lr_abd.n_iter_);\n",
    "print(\"bcd :\", lr_bcd.n_iter_);\n",
    "print();\n",
    "print(\"abcd :\", lr_abcd.n_iter_);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization (instead of default = L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ab = LogisticRegression(penalty = 'l1', solver = \"liblinear\", C=1.0);\n",
    "lr_ab.fit(Xs_ab, y);\n",
    "\n",
    "lr_bc = LogisticRegression(penalty = 'l1', solver = \"liblinear\", C=1.0);\n",
    "lr_bc.fit(Xs_bc, y);\n",
    "\n",
    "lr_cd = LogisticRegression(penalty = 'l1', solver = \"liblinear\", C=1.0);\n",
    "lr_cd.fit(Xs_cd, y);\n",
    "\n",
    "lr_ac = LogisticRegression(penalty = 'l1', solver = \"liblinear\", C=1.0);\n",
    "lr_ac.fit(Xs_ac, y);\n",
    "\n",
    "lr_ad = LogisticRegression(penalty = 'l1', solver = \"liblinear\", C=1.0);\n",
    "lr_ad.fit(Xs_ad, y);\n",
    "\n",
    "lr_bd = LogisticRegression(penalty = 'l1', solver = \"liblinear\", C=1.0);\n",
    "lr_bd.fit(Xs_bd, y);\n",
    "\n",
    "\n",
    "\n",
    "lr_abc = LogisticRegression(penalty = 'l1', solver = \"liblinear\", C=1.0);\n",
    "lr_abc.fit(Xs_abc, y);\n",
    "\n",
    "lr_acd = LogisticRegression(penalty = 'l1', solver = \"liblinear\", C=1.0);\n",
    "lr_acd.fit(Xs_acd, y);\n",
    "\n",
    "lr_abd = LogisticRegression(penalty = 'l1', solver = \"liblinear\", C=1.0);\n",
    "lr_abd.fit(Xs_abd, y);\n",
    "\n",
    "lr_bcd = LogisticRegression(penalty = 'l1', solver = \"liblinear\", C=1.0);\n",
    "lr_bcd.fit(Xs_bcd, y);\n",
    "\n",
    "\n",
    "\n",
    "lr_abcd = LogisticRegression(penalty = 'l1', solver = \"liblinear\", C=1.0);\n",
    "lr_abcd.fit(Xs_abcd, y);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies : \n",
      "accuaracy ab :  0.8\n",
      "accuaracy bc :  0.9133333333333333\n",
      "accuaracy cd :  0.9466666666666667\n",
      "accuaracy ac :  0.9466666666666667\n",
      "accuaracy ad :  0.9466666666666667\n",
      "accuaracy bd :  0.9466666666666667\n",
      "\n",
      "accuaracy abc :  0.9266666666666666\n",
      "accuaracy acd :  0.9466666666666667\n",
      "accuaracy abd :  0.9466666666666667\n",
      "accuaracy bcd :  0.9466666666666667\n",
      "\n",
      "accuaracy abcd :  0.94\n",
      "\n",
      "Number of Iterations : \n",
      "ab : [13]\n",
      "bc : [19]\n",
      "cd : [22]\n",
      "ac : [22]\n",
      "ad : [19]\n",
      "bd : [18]\n",
      "\n",
      "abc : [26]\n",
      "acd : [27]\n",
      "abd : [15]\n",
      "bcd : [21]\n",
      "\n",
      "abcd : [23]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracies : \");\n",
    "y_ab_hat = lr_ab.predict(Xs_ab);\n",
    "accuracy_ab = np.mean(y_ab_hat == y);\n",
    "print(\"accuaracy ab : \", accuracy_ab);\n",
    "\n",
    "y_bc_hat = lr_bc.predict(Xs_bc);\n",
    "accuracy_bc = np.mean(y_bc_hat == y);\n",
    "print(\"accuaracy bc : \", accuracy_bc);\n",
    "\n",
    "y_cd_hat = lr_cd.predict(Xs_cd);\n",
    "accuracy_cd = np.mean(y_cd_hat == y);\n",
    "print(\"accuaracy cd : \", accuracy_cd);\n",
    "\n",
    "y_ac_hat = lr_ac.predict(Xs_ac);\n",
    "accuracy_ac = np.mean(y_ac_hat == y);\n",
    "print(\"accuaracy ac : \", accuracy_ac);\n",
    "\n",
    "y_ad_hat = lr_ad.predict(Xs_ad);\n",
    "accuracy_ad = np.mean(y_ad_hat == y);\n",
    "print(\"accuaracy ad : \", accuracy_ad);\n",
    "\n",
    "y_bd_hat = lr_bd.predict(Xs_bd);\n",
    "accuracy_bd = np.mean(y_bd_hat == y);\n",
    "print(\"accuaracy bd : \", accuracy_bd);\n",
    "\n",
    "\n",
    "print();\n",
    "y_abc_hat = lr_abc.predict(Xs_abc);\n",
    "accuracy_abc = np.mean(y_abc_hat == y);\n",
    "print(\"accuaracy abc : \", accuracy_abc);\n",
    "\n",
    "y_acd_hat = lr_acd.predict(Xs_acd);\n",
    "accuracy_acd = np.mean(y_acd_hat == y);\n",
    "print(\"accuaracy acd : \", accuracy_acd);\n",
    "\n",
    "y_abd_hat = lr_abd.predict(Xs_abd);\n",
    "accuracy_abd = np.mean(y_abd_hat == y);\n",
    "print(\"accuaracy abd : \", accuracy_abd);\n",
    "\n",
    "y_bcd_hat = lr_bcd.predict(Xs_bcd);\n",
    "accuracy_bcd = np.mean(y_bcd_hat == y);\n",
    "print(\"accuaracy bcd : \", accuracy_bcd);\n",
    "\n",
    "print();\n",
    "y_abcd_hat = lr_abcd.predict(Xs_abcd);\n",
    "accuracy_abcd = np.mean(y_abcd_hat == y);\n",
    "print(\"accuaracy abcd : \", accuracy_abcd);\n",
    "\n",
    "print();\n",
    "print(\"Number of Iterations : \");\n",
    "print(\"ab :\", lr_ab.n_iter_);\n",
    "print(\"bc :\", lr_bc.n_iter_);\n",
    "print(\"cd :\", lr_cd.n_iter_);\n",
    "print(\"ac :\", lr_ac.n_iter_);\n",
    "print(\"ad :\", lr_ad.n_iter_);\n",
    "print(\"bd :\", lr_bd.n_iter_);\n",
    "print();\n",
    "print(\"abc :\", lr_abc.n_iter_);\n",
    "print(\"acd :\", lr_acd.n_iter_);\n",
    "print(\"abd :\", lr_abd.n_iter_);\n",
    "print(\"bcd :\", lr_bcd.n_iter_);\n",
    "print();\n",
    "print(\"abcd :\", lr_abcd.n_iter_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUMMARY :*** Compared to the logicstic regression model using L2 regularization, the results using L1 regularization seem to be more accurate and more iterations were needed for convergence. Also, the number of iterations seem to increase as one uses more features. \n",
    "\n",
    "It was ensured that when comparing the regression models, that the same C value and solver was used. Liblearn was used in both cases because it supports both L1 and L2 regularization. The C-value of 1 was used to specify stronger regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Parameter , C, Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights, params = [], []\n",
    "\n",
    "for c in np.arange(-20, 20):\n",
    "    lr = LogisticRegression(C=10.**c, random_state=0) # L2 reg\n",
    "    lr.fit(Xs_ab, y)\n",
    "    weights.append(lr.coef_[1]) # coef_ndarray of shape (1, n_features) or (n_classes, n_features)\n",
    "    params.append(10.**c)\n",
    "\n",
    "weights = np.array(weights)\n",
    "\n",
    "plt.plot(params, weights[:, 0],\n",
    "         label='sepal length')\n",
    "plt.plot(params, weights[:, 1], linestyle='--',\n",
    "         label='sepal width')\n",
    "plt.ylabel('weight coefficient')\n",
    "plt.xlabel('C')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is used to combat overfitting (high variance) in our data and is esssential i the *Bias - Variance Tradeoff* Low values of C indicate strong regulation and higher values of C inidcate lower regulation. So, based on this plot, the weight coefficients shrink if we decrease parameter C (increase the regularization strength)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization in Higher Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuaracy abcd :  0.9733333333333334\n",
      "abcd : [25]\n"
     ]
    }
   ],
   "source": [
    "lr_abcd = LogisticRegression(C=1.0);\n",
    "lr_abcd.fit(Xs_abcd, y);\n",
    "\n",
    "y_abcd_hat = lr_abcd.predict(Xs_abcd);\n",
    "accuracy_abcd = np.mean(y_abcd_hat == y);\n",
    "print(\"accuaracy abcd : \", accuracy_abcd);\n",
    "print(\"abcd :\", lr_abcd.n_iter_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuaracy abcd :  0.9866666666666667\n",
      "abcd : [55]\n"
     ]
    }
   ],
   "source": [
    "lr_abcd = LogisticRegression(C=1000.0); # higher C\n",
    "lr_abcd.fit(Xs_abcd, y);\n",
    "\n",
    "y_abcd_hat = lr_abcd.predict(Xs_abcd);\n",
    "accuracy_abcd = np.mean(y_abcd_hat == y);\n",
    "print(\"accuaracy abcd : \", accuracy_abcd);\n",
    "print(\"abcd :\", lr_abcd.n_iter_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing C (decreasing regularization strength) led to higher accuracy and number if iterations in the higher dimensional logistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuaracy cd :  0.9533333333333334\n",
      "cd : [18]\n"
     ]
    }
   ],
   "source": [
    "lr_cd = LogisticRegression(C=1.0);\n",
    "lr_cd.fit(Xs_cd, y);\n",
    "\n",
    "y_cd_hat = lr_cd.predict(Xs_cd);\n",
    "accuracy_cd = np.mean(y_cd_hat == y);\n",
    "print(\"accuaracy cd : \", accuracy_cd);\n",
    "print(\"cd :\", lr_cd.n_iter_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuaracy cd :  0.96\n",
      "cd : [26]\n"
     ]
    }
   ],
   "source": [
    "lr_cd = LogisticRegression(C=1000.0); # higher C\n",
    "lr_cd.fit(Xs_cd, y);\n",
    "\n",
    "y_cd_hat = lr_cd.predict(Xs_cd);\n",
    "accuracy_cd = np.mean(y_cd_hat == y);\n",
    "print(\"accuaracy cd : \", accuracy_cd);\n",
    "print(\"cd :\", lr_cd.n_iter_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing C (decreasing regularization strength) led to higher accuracy and number of iterations in the lower dimensional logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUMMARY :** All in all, regularization is important in increasing accuracy using logistic regession models. Is primary use is to prevent overfitting (high variance) in calssifying data. On the other side of the coin, underfitting (high bias) can be present. Underfitting suggests that the model is not complex enough to capture the pattern in the training data well and therefore suffers from low performance on unseen data. Regularization helps us find a good *bias-variance tradeoff*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
